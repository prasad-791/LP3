{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_GcJlVrDgBk"
   },
   "source": [
    "Name: **Prasad Khalkar**<br>\n",
    "Div: **BE-09 Q-09**<br>\n",
    "Roll no: **43138**<br>\n",
    "Title: **Assignment 5: Implement the Continuous Bag of Words (CBOW) Model**<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "V51q50EbF-T9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-07 10:02:59.850252: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-07 10:03:00.315530: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-07 10:03:00.315586: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-07 10:03:00.445500: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-07 10:03:04.814442: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-07 10:03:04.814540: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-07 10:03:04.814551: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "#importing libraries\n",
    "from keras.preprocessing import text\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import pad_sequences\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wBUwYdBJElVz"
   },
   "outputs": [],
   "source": [
    "#taking random sentences as data\n",
    "data = \"\"\"Deep learning (also known as deep structured learning) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised. \n",
    "Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning, recurrent neural networks, convolutional neural networks and Transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.\n",
    "\"\"\"\n",
    "dl_data = data.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "celNk9LmEvm8",
    "outputId": "4e2143b6-92dc-452f-f468-7c9e6238e287"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 75\n",
      "Vocabulary Sample: [('learning', 1), ('deep', 2), ('networks', 3), ('neural', 4), ('and', 5), ('as', 6), ('of', 7), ('machine', 8), ('supervised', 9), ('have', 10)]\n"
     ]
    }
   ],
   "source": [
    "#tokenization\n",
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(dl_data)\n",
    "word2id = tokenizer.word_index\n",
    "\n",
    "word2id['PAD'] = 0\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in dl_data]\n",
    "\n",
    "vocab_size = len(word2id)\n",
    "embed_size = 100\n",
    "window_size = 2 \n",
    "\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Vocabulary Sample:', list(word2id.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "AAxNYDanInQC"
   },
   "outputs": [],
   "source": [
    "#generating (context word, target/label word) pairs\n",
    "def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
    "    context_length = window_size*2\n",
    "    for words in corpus:\n",
    "        sentence_length = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            context_words = []\n",
    "            label_word   = []            \n",
    "            start = index - window_size\n",
    "            end = index + window_size + 1\n",
    "            \n",
    "            context_words.append([words[i] \n",
    "                                 for i in range(start, end) \n",
    "                                 if 0 <= i < sentence_length \n",
    "                                 and i != index])\n",
    "            label_word.append(word)\n",
    "\n",
    "            x = pad_sequences(context_words, maxlen=context_length)\n",
    "            y = np_utils.to_categorical(label_word, vocab_size)\n",
    "            yield (x, y)\n",
    "            \n",
    "i = 0\n",
    "for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
    "    if 0 not in x[0]:\n",
    "        # print('Context (X):', [id2word[w] for w in x[0]], '-> Target (Y):', id2word[np.argwhere(y[0])[0][0]])\n",
    "    \n",
    "        if i == 10:\n",
    "            break\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rb5dNmoZKZBv",
    "outputId": "b859c07e-6989-420d-b169-8aa0b93ff367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 4, 100)            7500      \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 75)                7575      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,075\n",
      "Trainable params: 15,075\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-07 10:03:31.343691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-07 10:03:31.344663: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-07 10:03:31.344859: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-11-07 10:03:31.345037: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-11-07 10:03:31.345214: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-11-07 10:03:31.345389: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-11-07 10:03:31.345735: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-11-07 10:03:31.345926: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-11-07 10:03:31.346103: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-11-07 10:03:31.346133: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-11-07 10:03:31.347417: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#model building\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda\n",
    "\n",
    "cbow = Sequential()\n",
    "cbow.add(Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=window_size*2))\n",
    "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))\n",
    "cbow.add(Dense(vocab_size, activation='softmax'))\n",
    "cbow.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "print(cbow.summary())\n",
    "\n",
    "# from IPython.display import SVG\n",
    "# from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "# SVG(model_to_dot(cbow, show_shapes=True, show_layer_names=False, rankdir='TB').create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xs12C3MDK1q4",
    "outputId": "fe8783b3-0ee1-4286-be40-6713afa14f9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tLoss: 433.8967480659485\n",
      "\n",
      "Epoch: 2 \tLoss: 428.96634697914124\n",
      "\n",
      "Epoch: 3 \tLoss: 425.404926776886\n",
      "\n",
      "Epoch: 4 \tLoss: 422.1164014339447\n",
      "\n",
      "Epoch: 5 \tLoss: 419.6993532180786\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 6):\n",
    "    loss = 0.\n",
    "    i = 0\n",
    "    for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
    "        i += 1\n",
    "        loss += cbow.train_on_batch(x, y)\n",
    "        if i % 100000 == 0:\n",
    "            print('Processed {} (context, word) pairs'.format(i))\n",
    "\n",
    "    print('Epoch:', epoch, '\\tLoss:', loss)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "id": "TZ3_QGKVK6Tj",
    "outputId": "cd9d167a-85a3-4cc9-eccb-faf997526122"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74, 100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>deep</th>\n",
       "      <td>-0.014699</td>\n",
       "      <td>0.065436</td>\n",
       "      <td>0.057544</td>\n",
       "      <td>-0.061038</td>\n",
       "      <td>-0.065660</td>\n",
       "      <td>0.010010</td>\n",
       "      <td>0.003206</td>\n",
       "      <td>0.060288</td>\n",
       "      <td>0.013841</td>\n",
       "      <td>-0.061265</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058907</td>\n",
       "      <td>-0.032038</td>\n",
       "      <td>0.028836</td>\n",
       "      <td>0.014327</td>\n",
       "      <td>-0.016924</td>\n",
       "      <td>-0.041142</td>\n",
       "      <td>0.003257</td>\n",
       "      <td>-0.023143</td>\n",
       "      <td>0.006548</td>\n",
       "      <td>0.028096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>networks</th>\n",
       "      <td>-0.000641</td>\n",
       "      <td>0.053669</td>\n",
       "      <td>0.041884</td>\n",
       "      <td>0.045035</td>\n",
       "      <td>-0.048811</td>\n",
       "      <td>0.043767</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.023994</td>\n",
       "      <td>0.022536</td>\n",
       "      <td>0.017235</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020328</td>\n",
       "      <td>-0.064310</td>\n",
       "      <td>-0.003517</td>\n",
       "      <td>0.025329</td>\n",
       "      <td>0.056089</td>\n",
       "      <td>0.008651</td>\n",
       "      <td>-0.015732</td>\n",
       "      <td>0.027755</td>\n",
       "      <td>0.013637</td>\n",
       "      <td>-0.043879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neural</th>\n",
       "      <td>0.017427</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>0.013169</td>\n",
       "      <td>-0.037246</td>\n",
       "      <td>-0.049257</td>\n",
       "      <td>0.030372</td>\n",
       "      <td>0.042760</td>\n",
       "      <td>0.014344</td>\n",
       "      <td>0.020658</td>\n",
       "      <td>0.039520</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014568</td>\n",
       "      <td>0.009679</td>\n",
       "      <td>0.027076</td>\n",
       "      <td>0.036409</td>\n",
       "      <td>0.046567</td>\n",
       "      <td>-0.027902</td>\n",
       "      <td>-0.025849</td>\n",
       "      <td>-0.048378</td>\n",
       "      <td>-0.043118</td>\n",
       "      <td>0.047227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>-0.032358</td>\n",
       "      <td>0.018356</td>\n",
       "      <td>-0.046243</td>\n",
       "      <td>-0.023890</td>\n",
       "      <td>0.049365</td>\n",
       "      <td>-0.019935</td>\n",
       "      <td>-0.026286</td>\n",
       "      <td>0.045641</td>\n",
       "      <td>-0.015162</td>\n",
       "      <td>0.027804</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>0.039208</td>\n",
       "      <td>-0.005720</td>\n",
       "      <td>-0.038928</td>\n",
       "      <td>-0.036059</td>\n",
       "      <td>0.032872</td>\n",
       "      <td>0.021264</td>\n",
       "      <td>-0.027767</td>\n",
       "      <td>0.011377</td>\n",
       "      <td>-0.038916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>as</th>\n",
       "      <td>0.046000</td>\n",
       "      <td>0.002624</td>\n",
       "      <td>-0.003722</td>\n",
       "      <td>0.031199</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>-0.040515</td>\n",
       "      <td>0.005326</td>\n",
       "      <td>0.020051</td>\n",
       "      <td>-0.034822</td>\n",
       "      <td>-0.008987</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023353</td>\n",
       "      <td>-0.039795</td>\n",
       "      <td>0.047382</td>\n",
       "      <td>-0.000363</td>\n",
       "      <td>0.038481</td>\n",
       "      <td>-0.015776</td>\n",
       "      <td>-0.006559</td>\n",
       "      <td>0.016380</td>\n",
       "      <td>-0.020997</td>\n",
       "      <td>0.015955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1         2         3         4         5   \\\n",
       "deep     -0.014699  0.065436  0.057544 -0.061038 -0.065660  0.010010   \n",
       "networks -0.000641  0.053669  0.041884  0.045035 -0.048811  0.043767   \n",
       "neural    0.017427  0.030400  0.013169 -0.037246 -0.049257  0.030372   \n",
       "and      -0.032358  0.018356 -0.046243 -0.023890  0.049365 -0.019935   \n",
       "as        0.046000  0.002624 -0.003722  0.031199  0.000486 -0.040515   \n",
       "\n",
       "                6         7         8         9   ...        90        91  \\\n",
       "deep      0.003206  0.060288  0.013841 -0.061265  ... -0.058907 -0.032038   \n",
       "networks  0.026316  0.023994  0.022536  0.017235  ... -0.020328 -0.064310   \n",
       "neural    0.042760  0.014344  0.020658  0.039520  ... -0.014568  0.009679   \n",
       "and      -0.026286  0.045641 -0.015162  0.027804  ...  0.000717  0.039208   \n",
       "as        0.005326  0.020051 -0.034822 -0.008987  ...  0.023353 -0.039795   \n",
       "\n",
       "                92        93        94        95        96        97  \\\n",
       "deep      0.028836  0.014327 -0.016924 -0.041142  0.003257 -0.023143   \n",
       "networks -0.003517  0.025329  0.056089  0.008651 -0.015732  0.027755   \n",
       "neural    0.027076  0.036409  0.046567 -0.027902 -0.025849 -0.048378   \n",
       "and      -0.005720 -0.038928 -0.036059  0.032872  0.021264 -0.027767   \n",
       "as        0.047382 -0.000363  0.038481 -0.015776 -0.006559  0.016380   \n",
       "\n",
       "                98        99  \n",
       "deep      0.006548  0.028096  \n",
       "networks  0.013637 -0.043879  \n",
       "neural   -0.043118  0.047227  \n",
       "and       0.011377 -0.038916  \n",
       "as       -0.020997  0.015955  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = cbow.get_weights()[0]\n",
    "weights = weights[1:]\n",
    "print(weights.shape)\n",
    "\n",
    "pd.DataFrame(weights, index=list(id2word.values())[1:]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UFs2IAn_LAYS",
    "outputId": "87ae1249-6a80-484f-b4a2-4d5f20734db7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74, 74)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'deep': ['design', 'performance', 'unsupervised', 'also', 'recurrent']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "distance_matrix = euclidean_distances(weights)\n",
    "print(distance_matrix.shape)\n",
    "\n",
    "similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:6]+1] \n",
    "                   for search_term in ['deep']}\n",
    "\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
